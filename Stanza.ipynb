{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "525de43c-fa5c-4cff-90e5-13f268f55e97",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/opt/anaconda3/envs/kg/lib/python3.8/site-packages/pyarrow/lib.cpython-38-darwin.so, 0x0002): Symbol not found: _deflate\n  Referenced from: <DED14171-1B72-379D-A906-F88314F29201> /opt/anaconda3/envs/kg/lib/libarrow.800.0.0.dylib\n  Expected in:     <47C6768E-CA90-32CD-87E6-865939F342B3> /opt/anaconda3/envs/kg/lib/liborc.dylib",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgliner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GLiNER\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kg/lib/python3.8/site-packages/datasets/__init__.py:22\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# pylint: enable=line-too-long\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,g-bad-import-order,wrong-import-position\u001b[39;00m\n\u001b[1;32m     20\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.14.4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kg/lib/python3.8/site-packages/datasets/arrow_dataset.py:59\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompute\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpc\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetCard, DatasetCardData, HfApi, HfFolder\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kg/lib/python3.8/site-packages/pyarrow/__init__.py:65\u001b[0m\n\u001b[1;32m     63\u001b[0m _gc_enabled \u001b[38;5;241m=\u001b[39m _gc\u001b[38;5;241m.\u001b[39misenabled()\n\u001b[1;32m     64\u001b[0m _gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_lib\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _gc_enabled:\n\u001b[1;32m     67\u001b[0m     _gc\u001b[38;5;241m.\u001b[39menable()\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/opt/anaconda3/envs/kg/lib/python3.8/site-packages/pyarrow/lib.cpython-38-darwin.so, 0x0002): Symbol not found: _deflate\n  Referenced from: <DED14171-1B72-379D-A906-F88314F29201> /opt/anaconda3/envs/kg/lib/libarrow.800.0.0.dylib\n  Expected in:     <47C6768E-CA90-32CD-87E6-865939F342B3> /opt/anaconda3/envs/kg/lib/liborc.dylib"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df97a253-0797-4358-b300-45bda6fba8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Path to your dataset file\n",
    "file_path = 'sample-1M.jsonl'\n",
    "\n",
    "# Read only the first 100 lines to save memory\n",
    "df = pd.read_json(file_path, lines=True, nrows=100)\n",
    "\n",
    "print(f\"Successfully loaded {len(df)} articles.\")\n",
    "if 'title' in df.columns:\n",
    "    print(\"Sample Article Title:\", df.loc[0, 'title'])\n",
    "else:\n",
    "    print(\"No 'title' column in the dataset\")\n",
    "\n",
    "# Display the first few rows\n",
    "# print(df.head())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d6bea7-60b6-416d-8d85-7f49d78b5c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # import pandas as pd\n",
    "\n",
    "# # ---------------------------------------------------------\n",
    "# # 1. Initialize Stanza Pipeline\n",
    "# # ---------------------------------------------------------\n",
    "# # 'processors': We only need tokenization and NER (Named Entity Recognition).\n",
    "# # 'use_gpu': Set to True if you have a GPU, otherwise False.\n",
    "# print(\"Initializing Stanza (downloading model if needed)...\")\n",
    "# stanza.download('en', processors='tokenize,ner')\n",
    "# nlp = stanza.Pipeline('en', processors='tokenize,ner', use_gpu=False, verbose=False)\n",
    "\n",
    "# # ---------------------------------------------------------\n",
    "# # 2. Define Entity Extraction Function\n",
    "# # ---------------------------------------------------------\n",
    "# def get_entities_stanza(text):\n",
    "#     \"\"\"\n",
    "#     Extracts specific entity types (PER, ORG, LOC, GPE) from text using Stanza.\n",
    "#     \"\"\"\n",
    "#     print(\"1\")\n",
    "#     if not isinstance(text, str) or not text.strip():\n",
    "#         return []\n",
    "    \n",
    "#     # Process text with Stanza\n",
    "#     print(\"Process text with NLP\")\n",
    "#     doc = nlp(text)\n",
    "    \n",
    "#     # Filter for relevant entity types\n",
    "#     # PERSON: People, including fictional.\n",
    "#     # ORG: Companies, agencies, institutions.\n",
    "#     # GPE: Countries, cities, states.\n",
    "#     # LOC: Non-GPE locations, mountain ranges, bodies of water.\n",
    "#     relevant_types = {'PERSON', 'ORG', 'GPE', 'LOC'}\n",
    "    \n",
    "#     entities = []\n",
    "#     # Stanza processes text sentence by sentence\n",
    "#     print(\"Stanza processes text sentence by sentence\")\n",
    "#     i = 1\n",
    "#     for sent in doc.sentences:\n",
    "#         # print(\"Sentence Number \", i)\n",
    "#         # print(\"Sentence : \", sent)\n",
    "#         for ent in sent.ents:\n",
    "#             # print(\"Entities :\", ent)\n",
    "#             if ent.type in relevant_types:\n",
    "#                 entities.append((ent.text, ent.type))\n",
    "#                 # print(\"Type : \", ent.type)\n",
    "#             # print(\"---------\")\n",
    "#         i = i + 1\n",
    "#     print(\"Entities :\", entities)\n",
    "#     return entities\n",
    "\n",
    "# # ---------------------------------------------------------\n",
    "# # 3. Process the DataFrame\n",
    "# # ---------------------------------------------------------\n",
    "# print(\"Extracting entities from dataframe...\")\n",
    "\n",
    "# # Create a combined text column (Title + Content) for better context\n",
    "# # Using .fillna('') to handle potential missing values\n",
    "# print(\"If content in df.columns\")\n",
    "# if 'content' in df.columns:\n",
    "#     df['full_text'] = df['title'].fillna('') + \". \" + df['content'].fillna('')\n",
    "# else:\n",
    "#     df['full_text'] = df['title'].fillna('')\n",
    "\n",
    "# # print(df['full_text'])\n",
    "\n",
    "# # Apply the extraction function (This may take a minute for 100 rows)\n",
    "# df['extracted_entities'] = df['full_text'].apply(get_entities_stanza)\n",
    "\n",
    "# # ---------------------------------------------------------\n",
    "# # 4. Display Results\n",
    "# # ---------------------------------------------------------\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"STANZA NER EXTRACTION RESULTS\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# # Display the first 5 results\n",
    "# for index, row in df.head(5).iterrows():\n",
    "#     print(f\"Article ID: {index}\")\n",
    "#     print(f\"Title: {row.get('title', 'No Title')}\")\n",
    "#     print(f\"Found Entities: {row['extracted_entities']}\")\n",
    "#     print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2477a79-e44d-4dbc-b44a-07d4b68461d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. LOAD GLiNER MODEL\n",
    "# ==========================================\n",
    "print(\"2. Loading GLiNER Model...\")\n",
    "# \"urchade/gliner_medium-v2.1\" is the best balance of speed vs accuracy\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_medium-v2.1\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. DEFINE EXTRACTION FUNCTION\n",
    "# ==========================================\n",
    "def extract_entities(text):\n",
    "    if not text: return []\n",
    "    \n",
    "    # DEFINE YOUR CUSTOM LABELS HERE\n",
    "    # GLiNER allows you to invent any label you want!\n",
    "    labels = [\"Person\", \"Company\", \"City\", \"Country\", \"Product\", \"Crime\"]\n",
    "    \n",
    "    entities = model.predict_entities(text, labels, threshold=0.3)\n",
    "    \n",
    "    # Return list of (Text, Label)\n",
    "    return [(e['text'], e['label']) for e in entities]\n",
    "\n",
    "# ==========================================\n",
    "# 4. RUN EXTRACTION\n",
    "# ==========================================\n",
    "print(\"3. Extracting entities...\")\n",
    "# Combine title and text for better context\n",
    "df['full_text'] = df['title'] + \". \" + df['text']\n",
    "df['entities'] = df['full_text'].apply(extract_entities)\n",
    "\n",
    "# ==========================================\n",
    "# 5. VIEW RESULTS\n",
    "# ==========================================\n",
    "for i, row in df.iterrows():\n",
    "    print(f\"\\n--- Article {i} ---\")\n",
    "    print(f\"TITLE: {row['title']}\")\n",
    "    print(f\"ENTITIES FOUND: {row['entities']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b843123-6692-49cb-b655-7064b64a8b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
